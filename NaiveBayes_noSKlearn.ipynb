{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.tokenize import word_tokenize \n",
    "from sklearn import linear_model\n",
    "import csv\n",
    "import string\n",
    "\n",
    "import io  \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))  \n",
    "\n",
    "\n",
    "size = 10\n",
    "\n",
    "mydict_real = {}\n",
    "ps = PorterStemmer() \n",
    "\n",
    "\n",
    "punctuation = \"!#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\n",
    "\n",
    "LOL_real=[]\n",
    "\n",
    "\n",
    "allTweet=[]\n",
    "df = pd.read_csv(filepath_or_buffer='data/tweets.csv')\n",
    "df_real = df.head(size)\n",
    "# df1 = df.head(size)\n",
    "# df2 = df.tail(size)\n",
    "# df = df1.append(df2)\n",
    "# allLabel = df['class'].array\n",
    "content = df_real['content']\n",
    "\n",
    "\n",
    "for line in content:\n",
    "    list = str(line).split(' ')\n",
    "    LOL_real.append(list)\n",
    "\n",
    "LOL_real_trim=[]    \n",
    "    \n",
    "for list in LOL_real:\n",
    "    newList=[]\n",
    "    for word in list:\n",
    "        if word in punctuation:\n",
    "            continue\n",
    "        trim_word = ps.stem(word).lower()\n",
    "        if trim_word in stop_words:\n",
    "            continue\n",
    "        if trim_word.isnumeric():\n",
    "            trim_word = \"number\"\n",
    "        newList.append(trim_word)\n",
    "    allTweet.append(newList)\n",
    "#         count = 0\n",
    "#         for one in LOL_real:\n",
    "#             print(one)\n",
    "#             print(trim_word)\n",
    "#             if trim_word in one:\n",
    "#         mydict_real[trim_word] = count\n",
    "\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "for list in allTweet:\n",
    "    for word in list:\n",
    "        count =0\n",
    "        for list in allTweet: \n",
    "            if word in list:\n",
    "                count +=1\n",
    "        mydict_real[word] = count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'@switchfoot': 1, 'http://twitpic.com/2y1zl': 1, 'awww,': 1, \"that'\": 1, 'bummer.': 1, 'shoulda': 1, 'got': 1, 'david': 1, 'carr': 1, 'third': 1, 'day': 1, 'it.': 1, ';d': 1, 'upset': 1, \"can't\": 2, 'updat': 1, 'hi': 1, 'facebook': 1, 'text': 1, 'it...': 1, 'might': 1, 'cri': 1, 'result': 1, 'school': 1, 'today': 1, 'also.': 1, 'blah!': 1, '@kenichan': 1, 'dive': 1, 'mani': 1, 'time': 2, 'ball.': 1, 'manag': 1, 'save': 1, '50%': 1, 'rest': 1, 'go': 1, 'bound': 1, 'whole': 2, 'bodi': 1, 'feel': 1, 'itchi': 1, 'like': 1, 'fire': 1, '@nationwideclass': 1, 'no,': 1, \"it'\": 1, 'behav': 1, 'all.': 1, \"i'm\": 2, 'mad.': 1, 'whi': 1, 'here?': 1, 'becaus': 1, 'see': 1, 'there.': 1, '@kwesidei': 1, 'crew': 1, 'need': 1, 'hug': 1, '@loltrish': 1, 'hey': 1, 'long': 1, 'see!': 1, 'yes..': 1, 'rain': 1, 'bit': 1, ',onli': 1, 'lol': 1, 'fine': 1, 'thank': 1, \"how'\": 1, '@tatiana_k': 1, 'nope': 1, '@twittera': 1, 'que': 1, 'muera': 1}\n"
     ]
    }
   ],
   "source": [
    "print(mydict_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['bloop.', 'whi', 'want', 'pussi', 'heaven', '\"ultim', 'goal\"', 'anyway', '�', 'https://t.co/c7rufljbok'], ['brooklyn', 'judg', 'rule', 'favor', 'anti-homeless', 'shelter', 'activist', 'https://t.co/uxagim4t4d', 'https://t.co/y6codorola'], ['http://t.co/q1jgvqldb', 'ef', 'crazy!!', 'great', 'exercis', 'though.', '@shaederring', '@thintine_em', '@pmsinglumberjak', '@jadesorrell_', '@ctrlaltmichaael'], ['might', 'workout', 'campu', 'thi', 'year', 'might.', '@concealedg_', '@buffahibuffalo', '@_rsavannah', '@austinlinehan', '@thandii4', '@gingerrea22', '@chrishana_mari'], ['white', 'hous', 'offer', 'health', 'care', 'bill', 'get', 'mix', 'review', 'https://t.co/07dammlqnr', 'https://t.co/60frf0hveh'], ['atlanta', 'polic', 'search', 'man', 'want', 'rape', 'https://t.co/jqogqqjdw', 'https://t.co/2co08golzr'], ['realli', \"fuckin'\", 'listen', 'number', 'music', 'submissions!!', 'submit', 'get', 'rotat', '-->', 'https://t.co/qeiw0jxutz'], ['thank', 'you!', 'love', 'travel', 'readers.', \"you'r\", 'great', 'father.', 'https://t.co/3slnlfkhxd'], ['never', 'call', 'back,', 'guess', 'im', 'annoy'], ['racism', 'ha', 'effect', 'proven', '@emmyjewel', 'go', 'sit', 'corner.', 'https://t.co/fbfaoi3huv']]\n"
     ]
    }
   ],
   "source": [
    "mydict_fake = {}\n",
    "\n",
    "LOL_fake=[]\n",
    "\n",
    "\n",
    "allTweet_fake=[]\n",
    "df_fake = df.tail(size)\n",
    "# df1 = df.head(size)\n",
    "# df2 = df.tail(size)\n",
    "# df = df1.append(df2)\n",
    "# allLabel = df['class'].array\n",
    "content = df_fake['content']\n",
    "\n",
    "\n",
    "for line in content:\n",
    "    list = str(line).split(' ')\n",
    "    LOL_fake.append(list)\n",
    "\n",
    "LOL_fake_trim=[]    \n",
    "    \n",
    "for list in LOL_fake:\n",
    "    newList=[]\n",
    "    for word in list:\n",
    "        if word in punctuation:\n",
    "            continue\n",
    "        trim_word = ps.stem(word).lower()\n",
    "        if trim_word in stop_words:\n",
    "            continue\n",
    "        if trim_word.isnumeric():\n",
    "            trim_word = \"number\"\n",
    "        newList.append(trim_word)\n",
    "    allTweet_fake.append(newList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "for list in allTweet_fake:\n",
    "    for word in list:\n",
    "        count =0\n",
    "        for list in allTweet_fake: \n",
    "            if word in list:\n",
    "                count +=1\n",
    "        mydict_fake[word] = count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bloop.': 1, 'whi': 1, 'want': 2, 'pussi': 1, 'heaven': 1, '\"ultim': 1, 'goal\"': 1, 'anyway': 1, '�': 1, 'https://t.co/c7rufljbok': 1, 'brooklyn': 1, 'judg': 1, 'rule': 1, 'favor': 1, 'anti-homeless': 1, 'shelter': 1, 'activist': 1, 'https://t.co/uxagim4t4d': 1, 'https://t.co/y6codorola': 1, 'http://t.co/q1jgvqldb': 1, 'ef': 1, 'crazy!!': 1, 'great': 2, 'exercis': 1, 'though.': 1, '@shaederring': 1, '@thintine_em': 1, '@pmsinglumberjak': 1, '@jadesorrell_': 1, '@ctrlaltmichaael': 1, 'might': 1, 'workout': 1, 'campu': 1, 'thi': 1, 'year': 1, 'might.': 1, '@concealedg_': 1, '@buffahibuffalo': 1, '@_rsavannah': 1, '@austinlinehan': 1, '@thandii4': 1, '@gingerrea22': 1, '@chrishana_mari': 1, 'white': 1, 'hous': 1, 'offer': 1, 'health': 1, 'care': 1, 'bill': 1, 'get': 2, 'mix': 1, 'review': 1, 'https://t.co/07dammlqnr': 1, 'https://t.co/60frf0hveh': 1, 'atlanta': 1, 'polic': 1, 'search': 1, 'man': 1, 'rape': 1, 'https://t.co/jqogqqjdw': 1, 'https://t.co/2co08golzr': 1, 'realli': 1, \"fuckin'\": 1, 'listen': 1, 'number': 1, 'music': 1, 'submissions!!': 1, 'submit': 1, 'rotat': 1, '-->': 1, 'https://t.co/qeiw0jxutz': 1, 'thank': 1, 'you!': 1, 'love': 1, 'travel': 1, 'readers.': 1, \"you'r\": 1, 'father.': 1, 'https://t.co/3slnlfkhxd': 1, 'never': 1, 'call': 1, 'back,': 1, 'guess': 1, 'im': 1, 'annoy': 1, 'racism': 1, 'ha': 1, 'effect': 1, 'proven': 1, '@emmyjewel': 1, 'go': 1, 'sit': 1, 'corner.': 1, 'https://t.co/fbfaoi3huv': 1}\n"
     ]
    }
   ],
   "source": [
    "print(mydict_fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob(word):\n",
    "    if word in mydict_real.keys(): \n",
    "        prob_real = mydict_real[word]\n",
    "    else: \n",
    "        prob_real = 0\n",
    "    if word in mydict_fake.keys(): \n",
    "        prob_fake = mydict_fake[word]\n",
    "    else: \n",
    "        prob_fake = 0\n",
    "    numerator = ((prob_fake/(size*2))*0.5)+1\n",
    "    denominator = ((prob_fake/(size*2))*0.5) + ((prob_real/(size*2))*0.5) +1\n",
    "#     if numerator ==0 and denominator ==0:\n",
    "#         return 0\n",
    "    return numerator/denominator\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 471,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob('kenonwino')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-466-9ab73e148374>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;36m0\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
